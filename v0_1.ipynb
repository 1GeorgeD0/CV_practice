{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ноутбук с первой формально обученной моделью. В качестве основы используется Resnet50, функция потерь - TripletLoss. Можем заметить, что метрики при тестировании достаточно низкие"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import motmetrics as mm\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "class UAVDataset(Dataset):\n",
    "    def __init__(self, root_dir, gt_dir, attr_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.gt_dir = gt_dir\n",
    "        self.attr_dir = attr_dir\n",
    "        self.transform = transform\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "        for seq in os.listdir(self.root_dir):\n",
    "            gt_file = os.path.join(self.gt_dir, f\"{seq}_gt.txt\")\n",
    "            attr_file = os.path.join(self.attr_dir, f\"{seq}_attr.txt\")\n",
    "            if not os.path.exists(gt_file) or not os.path.exists(attr_file):\n",
    "                continue\n",
    "            with open(gt_file, 'r') as f_gt, open(attr_file, 'r') as f_attr:\n",
    "                gt_lines = f_gt.readlines()\n",
    "                attr_lines = f_attr.readlines()\n",
    "            for gt_line, attr_line in zip(gt_lines, attr_lines):\n",
    "                frame_id, obj_id, x, y, w, h, score, in_view, occlusion = gt_line.strip().split(',')\n",
    "                if int(score) != 1:  # Учитываем только объекты с score = 1\n",
    "                    continue\n",
    "                attrs = list(map(int, attr_line.strip().split(',')))\n",
    "                img_path = os.path.join(self.root_dir, seq, f\"img{int(frame_id):06d}.jpg\")\n",
    "                data.append((img_path, int(obj_id), (float(x), float(y), float(w), float(h)), attrs))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, obj_id, bbox, attrs = self.data[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, obj_id, bbox, attrs\n",
    "\n",
    "class ReIDModel(nn.Module):\n",
    "    def __init__(self, embedding_size=128, pretrained=True):\n",
    "        super(ReIDModel, self).__init__()\n",
    "        self.backbone = models.resnet50(pretrained=pretrained)\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_features, embedding_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "def create_triplets(dataset, num_triplets=100):\n",
    "    triplets = []\n",
    "    for _ in range(num_triplets):\n",
    "        anchor_idx = torch.randint(0, len(dataset), (1,)).item()\n",
    "        anchor_img, anchor_id, _, _ = dataset[anchor_idx]\n",
    "        positive_indices = [i for i in range(len(dataset)) if dataset[i][1] == anchor_id]\n",
    "        if not positive_indices:\n",
    "            continue\n",
    "        positive_idx = torch.randint(0, len(positive_indices), (1,)).item()\n",
    "        positive_img, _, _, _ = dataset[positive_indices[positive_idx]]\n",
    "        negative_indices = [i for i in range(len(dataset)) if dataset[i][1] != anchor_id]\n",
    "        if not negative_indices:\n",
    "            continue\n",
    "        negative_idx = torch.randint(0, len(negative_indices), (1,)).item()\n",
    "        negative_img, _, _, _ = dataset[negative_indices[negative_idx]]\n",
    "        triplets.append((anchor_img, positive_img, negative_img))\n",
    "    return triplets\n",
    "\n",
    "def evaluate_reid_no_reference(model, test_loader):\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Сбор всех эмбеддингов и меток\n",
    "    with torch.no_grad():\n",
    "        for images, obj_ids, bboxes, attrs in test_loader:\n",
    "            embeddings = model(images).cpu().numpy()\n",
    "            all_embeddings.extend(embeddings)\n",
    "            all_labels.extend(obj_ids.cpu().numpy())\n",
    "\n",
    "    all_embeddings = np.array(all_embeddings)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Вычисление попарных расстояний между всеми эмбеддингами\n",
    "    distance_matrix = np.zeros((len(all_embeddings), len(all_embeddings)))\n",
    "    for i in range(len(all_embeddings)):\n",
    "        for j in range(len(all_embeddings)):\n",
    "            distance_matrix[i, j] = np.linalg.norm(all_embeddings[i] - all_embeddings[j])\n",
    "\n",
    "    # Вычисление Rank-1 и Rank-5\n",
    "    rank1 = 0\n",
    "    rank5 = 0\n",
    "    for i in range(len(all_embeddings)):\n",
    "        distances = distance_matrix[i]\n",
    "        sorted_indices = np.argsort(distances)  # Индексы объектов, отсортированных по расстоянию\n",
    "        sorted_labels = all_labels[sorted_indices]  # Соответствующие метки\n",
    "\n",
    "        # Rank-1: Первый объект в отсортированном списке\n",
    "        if sorted_labels[0] == all_labels[i]:\n",
    "            rank1 += 1\n",
    "\n",
    "        # Rank-5: Истинный объект в топ-5\n",
    "        if all_labels[i] in sorted_labels[:5]:\n",
    "            rank5 += 1\n",
    "\n",
    "    rank1_accuracy = rank1 / len(all_labels)\n",
    "    rank5_accuracy = rank5 / len(all_labels)\n",
    "\n",
    "    # Вычисление mAP (mean Average Precision)\n",
    "    average_precision = []\n",
    "    for i in range(len(all_embeddings)):\n",
    "        distances = distance_matrix[i]\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        sorted_labels = all_labels[sorted_indices]\n",
    "\n",
    "        # Вычисление Precision и Recall для каждого объекта\n",
    "        relevant = (sorted_labels == all_labels[i]).astype(int)\n",
    "        cumulative_relevant = np.cumsum(relevant)\n",
    "        precision = cumulative_relevant / (np.arange(len(relevant)) + 1)\n",
    "        recall = cumulative_relevant / np.sum(relevant)\n",
    "\n",
    "        # Вычисление Average Precision (AP)\n",
    "        ap = 0\n",
    "        for k in range(len(precision)):\n",
    "            if relevant[k]:\n",
    "                ap += precision[k]\n",
    "        ap /= np.sum(relevant)\n",
    "        average_precision.append(ap)\n",
    "\n",
    "    mAP = np.mean(average_precision)\n",
    "\n",
    "    # Вычисление Precision, Recall и F1-score\n",
    "    predicted_labels = []\n",
    "    for i in range(len(all_embeddings)):\n",
    "        distances = distance_matrix[i]\n",
    "        sorted_indices = np.argsort(distances)\n",
    "        predicted_label = all_labels[sorted_indices[1]]  # Берем ближайший объект (исключая себя)\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    precision = precision_score(all_labels, predicted_labels, average='macro')\n",
    "    recall = recall_score(all_labels, predicted_labels, average='macro')\n",
    "    f1 = f1_score(all_labels, predicted_labels, average='macro')\n",
    "\n",
    "    return {\n",
    "        \"Rank-1\": rank1_accuracy,\n",
    "        \"Rank-5\": rank5_accuracy,\n",
    "        \"mAP\": mAP,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-score\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    root_dir = r'E:\\Yandex\\UAV-benchmark-M'\n",
    "    gt_dir = r'E:\\Yandex\\UAV-benchmark-MOTD_v1.0\\GT'\n",
    "    attr_dir = r'E:\\Yandex\\M_attr\\train'\n",
    "    embedding_size = 128\n",
    "    batch_size = 32\n",
    "    num_epochs = 10\n",
    "    num_triplets = 1000\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((256, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = UAVDataset(root_dir=root_dir, gt_dir=gt_dir, attr_dir=attr_dir, transform=transform)\n",
    "triplets = create_triplets(dataset, num_triplets=num_triplets)\n",
    "\n",
    "model = ReIDModel(embedding_size=embedding_size, pretrained=True)\n",
    "criterion = nn.TripletMarginLoss(margin=1.0)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    for anchor, positive, negative in triplets:\n",
    "        optimizer.zero_grad()\n",
    "        anchor_emb = model(anchor.unsqueeze(0))\n",
    "        positive_emb = model(positive.unsqueeze(0))\n",
    "        negative_emb = model(negative.unsqueeze(0))\n",
    "        loss = criterion(anchor_emb, positive_emb, negative_emb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(triplets)}')\n",
    "\n",
    "torch.save(model.state_dict(), 'reid_model_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Rank-1': 1.0, 'Rank-5': 1.0, 'mAP': np.float64(0.8516212580102716), 'Precision': 0.2777777777777778, 'Recall': 0.3125, 'F1-score': 0.29411764705882354}\n"
     ]
    }
   ],
   "source": [
    "# Тестирование с использованием метрик MOT\n",
    "test_dataset = UAVDataset(root_dir=r'E:\\Yandex\\UAV-benchmark-M', gt_dir=r'E:\\Yandex\\UAV-benchmark-MOTD_v1.0\\GT', attr_dir=r'E:\\Yandex\\M_attr\\test', transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = ReIDModel(embedding_size=embedding_size, pretrained=True)\n",
    "model_path = 'reid_model_2.pth'\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "metrics = evaluate_reid_no_reference(model, test_loader)\n",
    "print(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
